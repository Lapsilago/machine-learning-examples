{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "Seting up an LSTM in `keras` is straightforward as `keras` has a pre-defined `LSTM` layer for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_steps = 1\n",
    "num_features = 2\n",
    "num_units = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(input_shape=(num_time_steps,num_features),\n",
    "               units=num_units,\n",
    "               activation='tanh',\n",
    "               recurrent_activation='sigmoid',\n",
    "               use_bias=True))\n",
    "model.compile(optimizer='adam', loss='MAE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 16\n",
      "Trainable params: 16\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.7879    , -0.7968972 ,  0.9924617 , -0.6057646 ],\n",
       "        [-0.14056396, -0.34560132,  0.9574034 ,  0.7960496 ]],\n",
       "       dtype=float32),\n",
       " array([[-0.0660542 , -0.32508007, -0.8477706 ,  0.41381747]],\n",
       "       dtype=float32),\n",
       " array([0., 1., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.array([[1,10],[2,20],[3,30]])\n",
    "x = np.array([[1, 10]]) # this creates an input layer\n",
    "X = x[np.newaxis,:,:] # usually multiple inputs are processed, which is why this additional axis is needed\n",
    "Y = model.predict(X)  # computes the actual result\n",
    "y = Y[0] # in this example, we only have 1 output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel, recurrent_kernel, bias = model.layers[0].get_weights()\n",
    "inputs = x\n",
    "activation = np.tanh\n",
    "recurrent_activation = lambda x : 1/(1+np.exp(-x))\n",
    "h_tm1 = 0\n",
    "c_tm1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_i = kernel[:, :num_units]\n",
    "kernel_f = kernel[:, num_units: num_units * 2]\n",
    "kernel_c = kernel[:, num_units * 2: num_units * 3]\n",
    "kernel_o = kernel[:, num_units * 3:]\n",
    "\n",
    "recurrent_kernel_i = recurrent_kernel[:, :num_units]\n",
    "recurrent_kernel_f = recurrent_kernel[:, num_units: num_units * 2]\n",
    "recurrent_kernel_c = recurrent_kernel[:, num_units * 2: num_units * 3]\n",
    "recurrent_kernel_o = recurrent_kernel[:, num_units * 3:]\n",
    "\n",
    "bias_i = bias[:num_units]\n",
    "bias_f = bias[num_units: num_units * 2]\n",
    "bias_c = bias[num_units * 2: num_units * 3]\n",
    "bias_o = bias[num_units * 3:]\n",
    "\n",
    "inputs_i = inputs\n",
    "inputs_f = inputs\n",
    "inputs_c = inputs\n",
    "inputs_o = inputs\n",
    "\n",
    "x_i = np.dot(inputs_i, kernel_i)\n",
    "x_f = np.dot(inputs_f, kernel_f)\n",
    "x_c = np.dot(inputs_c, kernel_c)\n",
    "x_o = np.dot(inputs_o, kernel_o)\n",
    "\n",
    "x_i = x_i + bias_i\n",
    "x_f = x_f + bias_f\n",
    "x_c = x_c + bias_c\n",
    "x_o = x_o + bias_o\n",
    "\n",
    "h_tm1_i = h_tm1\n",
    "h_tm1_f = h_tm1\n",
    "h_tm1_c = h_tm1\n",
    "h_tm1_o = h_tm1\n",
    "\n",
    "i = recurrent_activation(x_i + np.dot(h_tm1_i, recurrent_kernel_i))\n",
    "f = recurrent_activation(x_f + np.dot(h_tm1_f, recurrent_kernel_f))\n",
    "c = f * c_tm1 + i * activation(x_c + np.dot(h_tm1_c, recurrent_kernel_c))\n",
    "o = recurrent_activation(x_o + np.dot(h_tm1_o, recurrent_kernel_o))\n",
    "\n",
    "h = o * activation(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(h,Y, decimal=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.09993291]]), array([[0.0999329]], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_i: -2.193540\n",
      "x_f: -3.252910\n",
      "x_c: 10.566496\n",
      "x_o: 7.354731\n",
      "i: 0.100332\n",
      "f: 0.037222\n",
      "c: 0.100332\n",
      "o: 0.999361\n"
     ]
    }
   ],
   "source": [
    "print(\"x_i: %f\" % x_i)\n",
    "print(\"x_f: %f\" % x_f)\n",
    "print(\"x_c: %f\" % x_c)\n",
    "print(\"x_o: %f\" % x_o)\n",
    "\n",
    "print(\"i: %f\" % i)\n",
    "print(\"f: %f\" % f)\n",
    "print(\"c: %f\" % c)\n",
    "print(\"o: %f\" % o)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-8fc7537cb60c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-8fc7537cb60c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    https://keras.io/layers/recurrent/\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://keras.io/layers/recurrent/\n",
    "\n",
    "https://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "https://stackoverflow.com/questions/42861460/how-to-interpret-weights-in-a-lstm-layer-in-keras\n",
    "\n",
    "https://github.com/keras-team/keras/blob/master/keras/layers/recurrent.py#L1863\n",
    "\n",
    "http://deeplearning.net/tutorial/lstm.html\n",
    "\n",
    "https://stackoverflow.com/questions/51199753/extract-cell-state-lstm-keras\n",
    "\n",
    "https://stats.stackexchange.com/questions/221513/why-are-the-weights-of-rnn-lstm-networks-shared-across-time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
